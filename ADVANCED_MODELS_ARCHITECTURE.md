# Advanced Models Architecture - AI Studio v1.7.1

## âš ï¸ WICHTIG: Spezielle Komponenten fÃ¼r neue Modelle

Die neuen Modelle in v1.7.1 verwenden **NICHT** die Standard Stable Diffusion Architektur!

---

## ğŸ—ï¸ Architektur-Ãœbersicht

### **Standard SD/SDXL Modelle:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Standard Stable Diffusion Pipeline  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… CLIP Text Encoder                â”‚
â”‚ âœ… U-Net Denoising Model            â”‚
â”‚ âœ… VAE (Autoencoder)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Funktioniert mit:
- SD1.5
- SDXL
- SDXL-Turbo
- Pony Diffusion XL
- Illustrious XL
```

---

### **FLUX.1 Modelle - Spezielle Architektur:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLUX.1 Architecture (12B params)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… CLIP-L Text Encoder              â”‚
â”‚ âœ… T5-XXL Text Encoder (4.7B!)      â”‚
â”‚ âœ… Flux Transformer (DiT)           â”‚
â”‚ âœ… FLUX-VAE (eigener VAE)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Komponenten:
1. Text Encoders (Dual):
   - CLIP: openai/clip-vit-large-patch14
   - T5: google/t5-v1_1-xxl (4.7B Parameter!)

2. Transformer:
   - Rectified Flow Transformer
   - 12 Milliarden Parameter
   - MMDiT-Blocks (Multi-Modal)

3. VAE:
   - black-forest-labs/FLUX.1-dev (eigener VAE)
   - Nicht kompatibel mit SD-VAE!

VRAM: 24+ GB (wegen T5-XXL!)
```

**Hugging Face Pipeline:**
```python
from diffusers import FluxPipeline

# Korrekte Verwendung:
pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.float16
)

# Komponenten werden automatisch geladen:
# - text_encoder: CLIP-L
# - text_encoder_2: T5-XXL (4.7GB!)
# - transformer: Flux DiT
# - vae: FLUX-VAE
```

---

### **Wan Video Modelle - 3D Causal VAE:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wan Video Architecture (14B params) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… CLIP Text Encoder                â”‚
â”‚ âœ… T5 Text Encoder                  â”‚
â”‚ âœ… Diffusion Transformer (Video)    â”‚
â”‚ âœ… Wan-VAE (3D Causal VAE!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Komponenten:
1. Text Encoders:
   - CLIP fÃ¼r globales VerstÃ¤ndnis
   - T5 fÃ¼r detaillierte Prompts

2. Wan-VAE (3D Causal):
   - Model ID: wangkanai/wan22-vae
   - Speziell fÃ¼r Video-Generierung
   - 3D Causal Architecture
   - Nicht kompatibel mit Bild-VAEs!

3. Diffusion Transformer:
   - 14 Milliarden Parameter
   - Video-spezifisch
   - 3D Attention Mechanisms

VRAM: 16-24 GB
Generation: 5-15 Minuten pro Video
```

**Hugging Face Pipeline:**
```python
from diffusers import DiffusionPipeline

# Wan 2.1/2.2 korrekte Verwendung:
pipe = DiffusionPipeline.from_pretrained(
    "Wan-AI/Wan2.1-T2V-14B",
    torch_dtype=torch.float16,
    custom_pipeline="text_to_video_wan"  # Spezieller Pipeline!
)

# Separate VAE laden:
from diffusers import AutoencoderKL
vae = AutoencoderKL.from_pretrained(
    "wangkanai/wan22-vae",
    torch_dtype=torch.float16
)
pipe.vae = vae
```

---

### **Qwen-VL Modelle - Vision-Language:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen-VL Architecture (7B params)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Vision Encoder (ViT)             â”‚
â”‚ âœ… Qwen2.5 Language Model           â”‚
â”‚ âœ… Cross-Attention Layers           â”‚
â”‚ âœ… Image Tokenizer (eigener!)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Komponenten:
1. Vision Encoder:
   - Vision Transformer (ViT)
   - Verarbeitet Bilder zu Visual Tokens

2. Language Model:
   - Qwen2.5-7B Base Model
   - SentencePiece Tokenizer!
   - Nicht CLIP/T5!

3. Image Generation Module:
   - Qwen-Image spezifischer Decoder
   - Text-Rendering Spezialist
   - Eigene VAE

VRAM: 8-16 GB
Besonderheit: Chinesisch + Englisch
```

**Hugging Face Pipeline:**
```python
from transformers import Qwen2VLForConditionalGeneration

# Qwen-VL korrekte Verwendung:
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    torch_dtype=torch.float16
)

# Eigener Tokenizer erforderlich!
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct"
)
```

---

## ğŸ”§ Implementierungs-Anforderungen

### **Problem: Unser aktueller Code**

```python
# FALSCH fÃ¼r FLUX/Wan/Qwen:
"flux-dev": {
    "pipeline_class": DiffusionPipeline,  # Zu generisch!
    "img2img_class": None,
    "type": "text2img"
}
```

**Was fehlt:**
- âŒ Keine speziellen Pipeline-Klassen
- âŒ Keine VAE-Konfiguration
- âŒ Keine Text Encoder Konfiguration
- âŒ Keine T5-XXL Download-Warnung

---

## âœ… Korrekte Implementierung

### **FLUX.1 Models:**

```python
from diffusers import FluxPipeline, FluxImg2ImgPipeline

AVAILABLE_MODELS = {
    "flux-dev": {
        "name": "FLUX.1 Dev",
        "model_id": "black-forest-labs/FLUX.1-dev",
        "pipeline_class": FluxPipeline,
        "img2img_class": FluxImg2ImgPipeline,
        "type": "text2img",
        "requires_auth": True,  # Gated model!
        "components": {
            "text_encoder": "openai/clip-vit-large-patch14",
            "text_encoder_2": "google/t5-v1_1-xxl",  # 4.7GB!
            "vae": "black-forest-labs/FLUX.1-dev/vae",
            "transformer": "black-forest-labs/FLUX.1-dev/transformer"
        },
        "vram_required": 24,  # Minimum!
        "warning": "FLUX requires 24GB+ VRAM and HuggingFace login"
    }
}
```

---

### **Wan Video Models:**

```python
from diffusers import DiffusionPipeline
from diffusers import AutoencoderKL

AVAILABLE_MODELS = {
    "wan22-t2v": {
        "name": "Wan 2.2 T2V 14B",
        "model_id": "Wan-AI/Wan2.2-T2V-14B",
        "pipeline_class": DiffusionPipeline,
        "custom_pipeline": "text_to_video_wan",
        "type": "text2video",
        "components": {
            "vae": "wangkanai/wan22-vae",  # 3D Causal VAE!
            "text_encoder": "clip",
            "text_encoder_2": "t5"
        },
        "vram_required": 16,
        "generation_time": "5-15 minutes",
        "warning": "Video generation is experimental and very slow"
    }
}

# Spezielle Lade-Logik:
def load_wan_model(model_key):
    pipe = DiffusionPipeline.from_pretrained(
        model_info["model_id"],
        custom_pipeline="text_to_video_wan",
        torch_dtype=torch.float16
    )
    
    # Separate VAE laden:
    vae = AutoencoderKL.from_pretrained(
        "wangkanai/wan22-vae",
        torch_dtype=torch.float16
    )
    pipe.vae = vae
    
    return pipe
```

---

### **Qwen Models:**

```python
from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoTokenizer,
    AutoProcessor
)

AVAILABLE_MODELS = {
    "qwen": {
        "name": "Qwen-Image",
        "model_id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "pipeline_class": Qwen2VLForConditionalGeneration,
        "type": "text2img",
        "components": {
            "tokenizer": "Qwen/Qwen2.5-VL-7B-Instruct",
            "processor": "Qwen/Qwen2.5-VL-7B-Instruct"
        },
        "vram_required": 8,
        "special_features": [
            "Text rendering in images",
            "Chinese + English support",
            "Custom tokenizer required"
        ]
    }
}

# Spezielle Lade-Logik:
def load_qwen_model(model_key):
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        model_info["model_id"],
        torch_dtype=torch.float16
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_info["model_id"]
    )
    
    processor = AutoProcessor.from_pretrained(
        model_info["model_id"]
    )
    
    return model, tokenizer, processor
```

---

## ğŸ“¦ Requirements Update

### **ZusÃ¤tzliche Dependencies erforderlich:**

```python
# requirements.txt muss erweitert werden:

# FLUX.1 Support
transformers>=4.44.0  # FÃ¼r T5-XXL Support
accelerate>=0.33.0    # FÃ¼r groÃŸe Modelle

# Wan Video Support
# (Bereits mit diffusers abgedeckt)
# Aber: Separate VAE Download erforderlich!

# Qwen Support
sentencepiece>=0.2.1  # âœ… Bereits vorhanden
protobuf==5.28.3      # âœ… Bereits vorhanden
transformers>=4.44.0  # FÃ¼r Qwen2.5-VL

# Empfohlen fÃ¼r groÃŸe Modelle:
bitsandbytes>=0.43.0  # FÃ¼r 8-bit/4-bit Quantization
```

---

## âš ï¸ VRAM & Download-Warnungen

### **FLUX.1 Dev:**
```
Total Download: ~35 GB
â”œâ”€â”€ FLUX Transformer: ~23 GB
â”œâ”€â”€ T5-XXL Encoder: ~10 GB
â”œâ”€â”€ CLIP-L Encoder: ~1 GB
â””â”€â”€ FLUX-VAE: ~1 GB

VRAM Usage:
â”œâ”€â”€ FP16: ~22-24 GB
â”œâ”€â”€ INT8: ~12-14 GB (mit bitsandbytes)
â””â”€â”€ INT4: ~8-10 GB (mit bitsandbytes)
```

### **Wan 2.2 Models:**
```
Total Download: ~25 GB
â”œâ”€â”€ Wan Transformer: ~20 GB
â”œâ”€â”€ Wan-VAE: ~3 GB
â”œâ”€â”€ Text Encoders: ~2 GB

VRAM Usage:
â”œâ”€â”€ Video Generation: ~18-22 GB
â”œâ”€â”€ Image-to-Video: ~16-20 GB
â””â”€â”€ Generation Time: 5-15 minutes!
```

### **Qwen Models:**
```
Total Download: ~14 GB
â”œâ”€â”€ Qwen2.5-7B Model: ~13 GB
â”œâ”€â”€ Vision Encoder: ~1 GB

VRAM Usage:
â”œâ”€â”€ FP16: ~14-16 GB
â””â”€â”€ INT8: ~8-10 GB
```

---

## ğŸš§ Aktuelle EinschrÃ¤nkungen

### **Was NICHT funktioniert (noch):**

1. **FLUX.1 Models:**
   - âŒ Aktueller Code lÃ¤dt nur generisches `DiffusionPipeline`
   - âŒ T5-XXL wird nicht automatisch geladen
   - âŒ FLUX-VAE nicht konfiguriert
   - âŒ Keine Hugging Face Auth-PrÃ¼fung

2. **Wan Video Models:**
   - âŒ Kein `custom_pipeline` Parameter
   - âŒ Wan-VAE wird nicht separat geladen
   - âŒ Keine Video-Output-Handling
   - âŒ UI zeigt keine Video-Preview

3. **Qwen Models:**
   - âŒ Verwendet falsche Pipeline-Klasse
   - âŒ Kein SentencePiece Tokenizer geladen
   - âŒ Kein Processor konfiguriert
   - âŒ Image-Edit-Logik fehlt

---

## âœ… Empfohlene Vorgehensweise

### **Kurzfristig (v1.7.1):**
```
1. Dokumentation aktualisieren:
   âœ… ADVANCED_MODELS_ARCHITECTURE.md (diese Datei)
   âœ… Warnungen in README.md
   âœ… "Experimentell" Status fÃ¼r FLUX/Wan/Qwen

2. UI-Warnung hinzufÃ¼gen:
   - "Dieses Modell benÃ¶tigt spezielle Komponenten"
   - "Download-GrÃ¶ÃŸe: XX GB"
   - "VRAM erforderlich: XX GB"
   - "Hugging Face Login erforderlich" (FLUX)

3. Modelle als "Advanced" markieren:
   - Separate Kategorie in der UI
   - Klare Warnung vor dem Laden
```

### **Mittelfristig (v1.8.0):**
```
1. Spezielle Pipeline-Loader:
   - load_flux_model()
   - load_wan_model()
   - load_qwen_model()

2. Komponenten-Management:
   - Automatischer VAE-Download
   - Text Encoder Verwaltung
   - VRAM-Check vor dem Laden

3. UI-Verbesserungen:
   - Video-Player fÃ¼r Wan-Outputs
   - Download-Progress fÃ¼r groÃŸe Modelle
   - VRAM-Usage Anzeige
```

### **Langfristig (v2.0):**
```
1. VollstÃ¤ndige Integration:
   - Alle Komponenten automatisch
   - Optimierte Pipelines
   - Quantization-Support (8-bit/4-bit)

2. Video-Generation:
   - Dedicated Video Tab
   - Timeline-Editor
   - Frame-by-Frame Preview

3. Model-Varianten:
   - FLUX.1 schnell/dev/pro
   - Wan distilled models
   - Qwen verschiedene GrÃ¶ÃŸen
```

---

## ğŸ¯ FÃ¼r Entwickler

### **Wenn Du diese Modelle nutzen willst:**

#### **FLUX.1:**
```python
# Schritt 1: Hugging Face Login
from huggingface_hub import login
login(token="hf_...")

# Schritt 2: FLUX Pipeline laden
from diffusers import FluxPipeline
pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.float16
).to("cuda")

# Schritt 3: Generieren
image = pipe(
    "a cat",
    num_inference_steps=20,
    guidance_scale=3.5
).images[0]
```

#### **Wan 2.2:**
```python
# Schritt 1: Pipeline laden
from diffusers import DiffusionPipeline
pipe = DiffusionPipeline.from_pretrained(
    "Wan-AI/Wan2.2-T2V-14B",
    custom_pipeline="text_to_video_wan",
    torch_dtype=torch.float16
).to("cuda")

# Schritt 2: VAE laden
from diffusers import AutoencoderKL
vae = AutoencoderKL.from_pretrained(
    "wangkanai/wan22-vae",
    torch_dtype=torch.float16
).to("cuda")
pipe.vae = vae

# Schritt 3: Video generieren
video = pipe(
    "a cat playing",
    num_frames=120,  # 5 Sekunden @ 24fps
    num_inference_steps=30
).frames
```

#### **Qwen:**
```python
# Schritt 1: Model und Tokenizer laden
from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoTokenizer
)

model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    torch_dtype=torch.float16
).to("cuda")

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct"
)

# Schritt 2: Generieren
inputs = tokenizer("Generate an image of a cat", return_tensors="pt")
outputs = model.generate(**inputs)
image = decode_qwen_output(outputs)
```

---

## ğŸ“š WeiterfÃ¼hrende Links

### **FLUX.1:**
- Hugging Face: https://huggingface.co/black-forest-labs/FLUX.1-dev
- Diffusers Docs: https://huggingface.co/docs/diffusers/api/pipelines/flux
- Architecture Paper: https://arxiv.org/html/2507.09595v1

### **Wan:**
- GitHub: https://github.com/Wan-Video/Wan2.2
- Hugging Face: https://huggingface.co/Wan-AI
- VAE: https://huggingface.co/wangkanai/wan22-vae

### **Qwen:**
- GitHub: https://github.com/QwenLM/Qwen-Image
- Hugging Face: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
- Docs: https://qwenlm.github.io/

---

**Version:** v1.7.1  
**Status:** ğŸš§ Experimentelle UnterstÃ¼tzung - Spezielle Konfiguration erforderlich!  
**Letzte Aktualisierung:** 2025-01-20
